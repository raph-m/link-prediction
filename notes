Lien vers kaggle: https://www.kaggle.com/t/012200c0318541f6806bfe757092b4f0
Il faut citer des papiers si possible.

Il faudrait faire un plan d'attaque.
D'abord tester des choses très simples pour avoir des premiers résultats et une idée de la puissance de calcul nécessaire.

Le modèle "baseline" prend en compte seulement quelques features et les entraîne dans un SVM:
- number of overlapping words in paper titles
- number of common authors
- difference in publication years

Il faudrait créer ces features et ajouter quelques features au fur et à mesure.

brainstorming sur les features:
- mots les plus importants d'après TF-IDF dans le titre et dans l'abstract: check. voir comment on fait exactement. cosine distance ?
- biensûr les dates: à faire
- un genre d'historique de citation des auteurs. Par exemple les 10 auteurs les plus cités par les auteurs du texte: à faire
- représentation word2vec des abstracts ?
- représentations sous formes de graphes des textes et essayer d'en extraire des features


à faire:
relire les cours pour trouver tout ce qui pourrait nous servir.

brainstorming data exploration:
- nombre d'auteurs différents: check
- nombre d'apparitions d'un auteur dans la base de donnée: check
- les mots les plus fréquents dans les abstracts: pas encore
- distribution du nombre d'overlapping words chez les textes qui ne se citent pas et chez les textes qui se citent: check
- faire un tf-idf sur la base de donnée entière et voir les résultats de ça: à faire aussi
- combien de journaux différents ?
- combien d'auteurs manquants ?

subsampler pour l'exploration, les premiers tests ?

brainstorming recherche d'articles:
- checker dans les cours les articles qui sont cités
- demander à des gens ?
- faire des recherches en ligne



Références à aller checker:
• Christopher	D.	Manning,	Prabhakar Raghavan and	Hinrich
Schütze,	Introduction	to	Information	Retrieval,	Cambridge
University	Press.	2008.	http://www-nlp.stanford.edu/IR-book/
• “Indexing	by	Latent	Semantic	Analysis”,	S.Deerwester,
S.Dumais,	T.Landauer,	G.Fumas,	R.Harshman,	Journal	of	the
Society	for	Information	Science,	1990
• “Mining	the	Web:	Discovering	Knowledge	from	Hypertext
Data”,	Soumen	Chakrabarti



Résultats:
Random Forest avec les paramètres de base:
Avec 10 estimateurs j'ai peu d'overfitting et un score de 0.85. Avec 30 estimateurs j'ai pas franchement plus d'overfitting mais les résultats ne s'améliorent pas.
Si je rajoute cosine distance avec 30 estimateurs j'ai masse overfitting et les résultats qui baissent. Avec 10 estimateurs ça overfitte encore pas mal. Donc faudrait réussir à réduire cet overfitting
Si je rajoute les deux score en plus, on reste sur de l'overfitting de gros porc.

Light GBM:
pas d'overfitting sur les features de base. Léger overfitting avec la cosine distance mais les résultats sont meilleurs, genre 0.87. Ensuite rajouter les deux scores n'améliore pas vraiment les résultats.

Avec shortest path on arrive à 94.5/93.8 (train/test).
En LGBM on a des résultats un peu meilleurs et avec moins d'overfitting.

On passe à 94.1/94.1 si on rajoute cosine distance et l'ajout de cosine distance a un intérêt très limité...

LGBM avec les basics et shortest path:  92.9/92.8
LGBM avec shortest path et overlapping: 94.2/93.9
RF avec shortest path et overlapping:   94.5/93.7
LGBM avec shortest path et cosine distance: 94.2/93.9


Un papier qui fait de la link prediction (coauthorship)
http://www.cs.rpi.edu/~zaki/PaperDir/LINK06.pdf
Un article qui donne des bonnées idées sur la théorie des graphes:
http://be.amazd.com/link-prediction/

Une thèse sur les graphes dirigés:
https://www.cs.upc.edu/~dariog/PhD-Thesis-Link-Prediction-DGG.pdf


relecture de code le 1er Mars:
_ je change un peu le format du code. Maintenant c'est en mode projet donc tu dois ouvrir tout le bordel sous pycharm et changer les paramètres pour que le working directory ça soit toujours la source du projet
- preprocessing. done.
- pour moi dans author_graph_features il y a un soucis. Le même que ce qu'on avait déjà eu avant, il faudrait supprimer les arrêtes qui existent si target == 1. à corriger ou à jeter... Si tu le corriges je peux le faire tourner sur Compute Engine.
-